
What needs to be improved or refactored in your Spark job:

1. class config for all filds 

already present. 
case class AppConfig(
    sourceDir: String = "data/input",
    destDir: String = "data/output",
    delimiter: String = ",",
    dedupKey: String = "id",
    partitionCol: String = "processing_timestamp"
  )
in main code. 
and section in config: 
"sourceDir = "data/input"
  destDir = "data/output"
  delimiter = ","
  dedupKey = "id"
  partitionColumn = "processing_timestamp"
"
also schema: 
"
schemaMapping {
    id = "IntegerType"
    name = "StringType"
    price = "DoubleType"
    age = "LongType"
    height = "FloatType"
    is_active = "BooleanType"
    created_date = "DateType:yyyy-MM-dd"
    updated_at = "TimestampType:yyyy-MM-dd HH:mm:ss"
    balance = "DecimalType:10,2"
  }
"

2. schema with types and configurations for casting
it's been already presented. 

3. csv without header (how to work with it)
done
4. .option("inferSchema", "true") -> false
done
5. change DROPMALFORMED to another options for malformed handling (save wrong lines (malformed))
done
6. option "overwrite" need to be chages to "append" in order to work with partitions