# ğŸš— Spark CSV â†’ Avro Converter

A **Dockerized Apache Spark job** (Scala 2.12, Spark 3.5) that reads CSV files, validates data, casts columns, removes duplicates, and writes the processed data in **Avro format**.

## ğŸ§± Tech Stack
- **Language:** Scala 2.12  
- **Framework:** Apache Spark 3.5  
- **Input:** CSV  
- **Output:** Avro  
- **Configuration:** HOCON (`application.conf`)  
- **Build Tool:** sbt  
- **Containerization:** Docker + Docker Compose  

---

## ğŸ“ Project Structure

```
spark-csv-to-avro/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/                      # Input CSV files
â”‚   â”œâ”€â”€ output/                     # Generated Avro files
â”‚   â””â”€â”€ scripts/                    # Spark Scala scripts (e.g. check_avro.scala)
â”œâ”€â”€ src/                            # Scala source code
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ build.sbt
â”œâ”€â”€ check_avro_data.sh              # Script to inspect Avro data
â””â”€â”€ README.md
```

---

## ğŸ‘¤ Spark User and File Permissions

The Spark container runs under **user ID `185` (spark)** for security and reproducibility.  
To ensure proper read/write access, make sure the `data/` directory is owned by your local user.

Run this once before starting the project:

```bash
sudo chown -R $USER:$USER data
chmod -R 775 data
```

This ensures both your local user and the containerized Spark process can read/write files in `data/input` and `data/output`.

---

## â–¶ï¸ Run Spark Job

To build and start the Spark job:

```bash
docker compose up --build
```

This command:
- Builds the Spark image
- Mounts the `data/` directory
- Executes the Scala Spark job that processes CSV â†’ Avro

---

## ğŸ” Check Avro Data

You can inspect an Avro output file directly using the helper script:

```bash
bash check_avro_data.sh 2025-10-24
```

This will:
- Generate a temporary Scala script (`data/scripts/check_avro.scala`)
- Launch a Spark container with `spark-shell`
- Load the Avro file from `data/output/processing_date=2025-10-24`
- Display the schema and sample data

Example output:

```
===== DATA PREVIEW =====
+---+-----+-------+------------+
|id |name |price  |created_date|
+---+-----+-------+------------+
|1  |Car A|12000.5|2025-10-20  |
|2  |Car B|13500.0|2025-10-21  |
|3  |Car C|14000.0|2025-10-22  |
+---+-----+-------+------------+

===== SCHEMA =====
root
 |-- id: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- price: double (nullable = true)
 |-- created_date: date (nullable = true)
```

---

## ğŸ§© Notes
- All Spark code runs inside the container â€” no Spark installation required locally.
- Data persistence between runs is handled through the `data/` volume.
- You can freely modify and re-run the CSV â†’ Avro conversion pipeline.

---